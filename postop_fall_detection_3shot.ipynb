{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15197ac0-231d-4a1a-b24e-950fba4c72a3",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting: Fall Detection with Open-Source Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6da88b-5378-4f32-b324-34875889a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadc70e-7bd5-4590-9e32-b1a6dc6d0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "dataset = pd.read_json('INSERT DATASET PATH')\n",
    "# required columns:\n",
    "## pat_deid: patient ID\n",
    "## note_deid: note ID\n",
    "## effective_time: note time\n",
    "## min_surg_date: surgery date\n",
    "## regex_chunked_note: note chunked with regular expression protocol\n",
    "## label: binary label (fall, no fall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a4683-3700-464f-b6f6-062d48455534",
   "metadata": {},
   "source": [
    "## Model, Prompt, and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258af7e-8f62-4afb-a31c-21e5fa2e130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mixtral:8x7b\"\n",
    "# other models used: \"gemma:7b\", \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff864d-eae6-4ea0-b873-3add29d13786",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=model_name,\n",
    "             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "             temperature=0.0,\n",
    "             top_p=0.25,\n",
    "             num_predict=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659f26e-f9ae-44d3-b458-dc8655de68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The few-shot prompt examples were removed as they contained protected health information. Please replace the [BRACKETED TEXT] with your examples.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"note\"],\n",
    "    template = \"\"\"You are a clinician who is reading a clinical note and looking for fall events. You are noting whether a patient fell or not after surgery. Please output 1 if the patient fell or 0 if the patient did not fall. Please note that historical falls, fall risk/precautions, or other miscellaneous mentions of falls like blood pressure falling are not fall events and the output should be 0 unless a fall event is also indicated in the note. \n",
    "\n",
    "    Here is an example of a note containing a fall event after surgery: \n",
    "    [INSERT NOTE EXAMPLE HERE]\n",
    "    EXPLANATION: [INSERT EXAMPLE EXPLANATION HERE].; OUTPUT: 1.\n",
    "    \n",
    "    Here is an example of a note containing a fall event: \n",
    "    [INSERT NOTE EXAMPLE HERE]\n",
    "    EXPLANATION: [INSERT EXAMPLE EXPLANATION HERE.]; OUTPUT: 1.\n",
    "    \n",
    "    Here is an example of a note that does not contain a fall event: \n",
    "    [INSERT NOTE EXAMPLE HERE]\n",
    "    EXPLANATION: [INSERT EXAMPLE EXPLANATION HERE.]; OUTPUT: 0.\n",
    "    \n",
    "    Please provide your response in the following format- EXPLANATION: ; OUTPUT: .\n",
    "    Clinical note: {note}. \n",
    "    Response: \n",
    "    \"\"\",\n",
    "    stopwords=[\"\\n\"],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c9861-a9c2-4510-8bcd-fe491c227c3f",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783809f9-661a-4689-853c-27ca13cc07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes each note and appends a sentence stating when a note was written and when the patient had surgery. It then runs the full prompt through the chain.\n",
    "def detect_falls(prompt, note_date, surg_date, note):\n",
    "    timing = \"This note was written on %s. The patient had surgery on %s. \" % (note_date, surg_date)\n",
    "    prompt_note = timing+note\n",
    "    \n",
    "    response=chain.invoke(prompt_note)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bffb2-1a1f-4ef5-96ba-afb3a0e93ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts the output from the model response.\n",
    "def extract_label(response):\n",
    "    pattern = r\"OUTPUT: [01]\"\n",
    "    matches = re.finditer(pattern, response.upper())\n",
    "\n",
    "    results=[]\n",
    "    for match in matches:\n",
    "        results.append((match.group(), match.start())) \n",
    "        # [('OUTPUT: 0', 140), ('OUTPUT: 1', 332)]\n",
    "        \n",
    "    if not results:\n",
    "        label = response\n",
    "        print(label)\n",
    "    else:\n",
    "        label = int(results[-1][0][-1])\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f902b-2ef9-4b00-9355-5d4c8df7eb90",
   "metadata": {},
   "source": [
    "## Few-shot Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2b344-4876-4259-8577-5232f90049ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the notes through the chain once. Please adapt for running 5 times as was described in the manuscript.\n",
    "\n",
    "start_time = time.time()\n",
    "col_name = 'mixtral_7b_response'\n",
    "for ind, x in dataset.iterrows():\n",
    "    print(ind, '--- ')\n",
    "    s_time = time.time()\n",
    "    output = detect_falls(prompt, x['effective_time'], x['surg_date'], x['regex_chunked_note'])\n",
    "    dataset.loc[ind, col_name] = output\n",
    "    print('-->', time.time() - s_time)\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Run took {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd65f4-7327-4288-aa20-56810e0932b3",
   "metadata": {},
   "source": [
    "### Output Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f293f40-95d0-4726-a5b7-69407dbf6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['mixtral_7b_label'] = dataset['mixtral_7b_response'].apply(extract_label)\n",
    "dataset['mixtral_7b_label'] = dataset['mixtral_7b_label'].astype(int)\n",
    "dataset['mixtral_7b_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289af0c8-e028-4c37-9bff-7ec6bc0d74b9",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98deef-504a-45d8-a7e6-e0452ea5e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(chunk_results.label, chunk_results.mixtral_7b_label))\n",
    "print(roc_auc_score(chunk_results.label, chunk_results.mixtral_7b_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falls_llm",
   "language": "python",
   "name": "falls_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
